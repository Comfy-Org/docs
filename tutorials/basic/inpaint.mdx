---
title: ComfyUI Inpainting Workflow
description: This guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor
sidebarTitle: "Inpaint"
---

This article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. We'll cover:
- Using inpainting workflows to modify images
- Using the ComfyUI mask editor to draw masks
- Learning about the **VAE Encoder (for Inpainting)** node

## About Inpainting

In AI image generation, we often encounter situations where we're satisfied with the overall image but there are elements we don't want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful.

It's like having an **artist (AI model)** paint a picture, but we're still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements.

Common inpainting scenarios include:
- **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.
- **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)
- And other scenarios

## ComfyUI Inpainting Workflow Example

### Model and Resource Preparation

#### 1. Inpainting Models

In this tutorial, you can use any of the following models, or continue using the SD1.5 model we used in previous [Text-to-Image](/tutorials/basic/text-to-image) and [Image-to-Image](/tutorials/basic/image-to-image) tutorials:
- [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)
- [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)
- [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)

After downloading, save the model file to the `ComfyUI/models/checkpoints` directory. You can also create subdirectories for different model types like **SD1.5**, **SD2.0**, **SDXL**, etc:
- `ComfyUI/models/checkpoints/SD1.5`
- `ComfyUI/models/checkpoints/SD2.0`
- `ComfyUI/models/checkpoints/SDXL`

These directories will still be recognized by ComfyUI. As you progress through our tutorials, you'll encounter different types of models that may not be fully compatible with each other, so using folders helps better organize different model types for future use.

#### 2. Input Image

Please download the image below, which we'll use as input for this example

![ComfyUI Inpainting Input Image](/images/tutorial/basic/inpaint/yosemite_inpaint_example.png)

<Note>This photo already includes an alpha transparency channel, so you don't need to manually draw a mask. This tutorial will also cover how to use the mask editor to draw masks, and we'll guide you step by step through the entire inpainting process</Note>

#### 3. Inpainting Workflow

Please download the image below and either **drag it** into the ComfyUI interface or use the menu **Workflow** --> **Open (shortcut `Ctrl + O`)** to load this inpainting workflow

![ComfyUI Inpainting Workflow](/images/tutorial/basic/inpaint/inpain_model_cat.png)

### ComfyUI Inpainting Workflow Example Explanation

![ComfyUI Inpainting Workflow](/images/tutorial/basic/inpaint/inpaint_workflow.jpg)

Please follow these steps according to the numbered image:
1. Make sure you've loaded your downloaded model
2. Load your inpainting source material into the `Load Image` node
3. Click the `Queue` button or use the shortcut `Ctrl + Enter` to generate the image

You might find that the results aren't perfect - for example, the generated cat might still have some defects, with oddly structured eyes

![ComfyUI Inpainting Workflow - Output Result](/images/tutorial/basic/inpaint/inpaint_output.jpg)

Remember our analogy? Different models are like artists with different capabilities, each with their own limitations. The current SD1.5 model has its constraints. However, as the open-source community evolves, models like SDXL and Flux are becoming more capable, with some able to generate high-quality images in one go, though they require more powerful hardware.

You can try these approaches to achieve better results:

1. Modify positive and negative prompts with more specific descriptions
2. Try multiple runs using different seeds in the `KSampler` for different generation results
3. After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.

Next, we'll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit), so manual mask drawing isn't necessary, you'll often use the Mask Editor to create masks in practical applications.

### Using the Mask Editor

First, let's load our unsatisfactory generated image result back into the `Load Image` node

![Reloading the Image](/images/tutorial/basic/inpaint/inpaint_send_to_workflow.jpg)

1. You can save the image locally and re-upload it
2. Or use the `Send to workflow` --> `Current Workflow` option in the `Save Image` node's right-click menu to reload the inpainted image.

Right-click on the **Load Image** node and select `Open in MaskEditor` from the context menu

![Opening the Mask Editor](/images/tutorial/basic/inpaint/inpint_open_in_maskeditor.jpg)

![Mask Editor](/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)

1. You can edit related parameters on the right, such as adjusting brush size, transparency, etc.
2. Use the eraser to remove incorrectly drawn areas
3. Click the `Save` button to save your mask

The drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding

Then try adjusting your prompts and generating again until you achieve satisfactory results.

## Inpainting-Related Nodes

Comparing this workflow with [Text-to-Image](/tutorials/basic/text-to-image) and [Image-to-Image](/tutorials/basic/image-to-image), you'll notice the main differences are in the VAE section's conditional inputs.
In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results.

![VAE Encoder (for Inpainting) Node](/images/comfy_core/lantent/inpaint/vae_encode_for_inpainting.jpg)

**Input Types**

| Parameter Name  | Function                                                     |
|----------------|--------------------------------------------------------------|
| `pixels`       | Input image to be encoded into latent space. |
| `vae`          | VAE model used to encode the image from pixel space to latent space. |
| `mask`         | Image mask specifying which areas need modification. |
| `grow_mask_by` | Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas. |

**Output Types**

| Parameter Name | Function                        |
|---------------|--------------------------------|
| `latent`      | Image encoded into latent space by the VAE. |
