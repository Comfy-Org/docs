---
title: "Wan2.2 Animate ComfyUI native workflow"
description: "Unified character animation and replacement framework with precise motion and expression replication."
sidebarTitle: "Wan2.2 Animate"
---
import UpdateReminder from '/snippets/tutorials/update-reminder.mdx'

Wan-Animate is a unified framework for character animation and replacement developed by the WAN Team.

The model can animate any character based on a performerâ€™s video, precisely replicating the performerâ€™s facial expressions and movements to generate highly realistic character videos.

It can also replace characters in a video with animated characters, preserving their expressions and movements while replicating the original lighting and color tone for seamless environmental integration.

## Model Highlights

- Dual Mode Functionality: A single architecture supports both animation and replacement functions, enabling easy operation switching.
- Advanced Body Motion Control: Uses spatially-aligned skeleton signals for accurate body movement replication
- Precise Motion and Expression: Accurately reproduces the movements and facial expressions from the reference video.
- Natural Environment Integration: Seamlessly blends the replaced character with the original video environment.
- Smooth Long Video Generation: Iterative generation ensures consistent motion and visual flow in extended videos

<UpdateReminder/>


## About Wan2.2 Animate workflow

In this docs, we will provide two workflow:

1. Workflow that only uses core nodes (It is incomplete; you need to preprocess the image by yourself first)
2. Workflow that inculdes some custom nodes (It is complete; you can use it directly, but some new user might not know how to install the custom nodes)

## Wan2.2 Anmate ComfyUI native workflow(without custom nodes)

### 1. Download Workflow File

Download the following workflow file and drag it into ComfyUI to load the workflow.

Download materials below as input:

**Reference image:**

![Reference_Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_animate/ref_image.png)

**Pose video:**

![Pose_Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_animate/pose_video.mp4)

**Face video:**

![Face_Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_animate/face_video.mp4)

Face video:

### 2. Model links


**diffusion_models**       
- [Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors](https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/Wan22Animate/Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors) This is the model that from Kijai's repo
- [wan2.2_animate_14B_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_animate_14B_bf16.safetensors) original model weight

**clip_visions**
- [clip_vision_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)

**vae**
- [wan_2.1_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**text_encoders**   
- [umt5_xxl_fp8_e4m3fn_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)


```
ComfyUI/
â”œâ”€â”€â”€ðŸ“‚ models/
â”‚   â”œâ”€â”€â”€ðŸ“‚ diffusion_models/
â”‚   â”‚   â”œâ”€â”€â”€ Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors
â”‚   â”‚   â””â”€â”€â”€ wan2.2_animate_14B_bf16.safetensors
â”‚   â”œâ”€â”€â”€ðŸ“‚ text_encoders/
â”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors 
â”‚   â”œâ”€â”€â”€ðŸ“‚ clip_visions/ 
â”‚   â”‚   â””â”€â”€â”€ clip_vision_h.safetensors
â”‚   â””â”€â”€â”€ðŸ“‚ vae/
â”‚       â””â”€â”€ wan_2.1_vae.safetensors
```

### 3. Workflow Instructions

![Workflow Instructions](/images/tutorial/video/wan/wan2_2/wan_2.2_14B_animate-navite-move.jpg)

1. Make sure all the models are loaded correctly
2. Upload the reference image, the character is this image will be the target character
3. You can use the videos we provided as input videos for the first time,
then you might need the **DWPose Estimator** node  in [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to help you detect the face and pose(Check the later section for more details)
  - Face video: use to control character's Facial expressions and the head
  - Pose video: use to control character's body motion
4. 

## Wan2.2 Anmate ComfyUI native workflow(with custom nodes)

### 1. Download Workflow File

Download the following workflow file and drag it into ComfyUI to load the workflow, if you have [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) installed, you can just click the `Install missing nodes` button to install the missing nodes.

### 2. Custom nodes

We need to install the following custom nodes:

- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)
- [ComfyUI-comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

If you don't know how to install custom nodes please refer to [How to install custom nodes](/installation/install_custom_node)

### 3. Workflow Instructions

