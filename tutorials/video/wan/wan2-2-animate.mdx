---
title: "Wan2.2 Animate ComfyUI native workflow"
description: "Unified character animation and replacement framework with precise motion and expression replication."
sidebarTitle: "Wan2.2 Animate"
---
import UpdateReminder from '/snippets/tutorials/update-reminder.mdx'

Wan-Animate is a unified framework for character animation and replacement developed by the WAN Team.

The model can animate any character based on a performerâ€™s video, precisely replicating the performerâ€™s facial expressions and movements to generate highly realistic character videos.

It can also replace characters in a video with animated characters, preserving their expressions and movements while replicating the original lighting and color tone for seamless environmental integration.

## Model Highlights

- Dual Mode Functionality: A single architecture supports both animation and replacement functions, enabling easy operation switching.
- Advanced Body Motion Control: Uses spatially-aligned skeleton signals for accurate body movement replication
- Precise Motion and Expression: Accurately reproduces the movements and facial expressions from the reference video.
- Natural Environment Integration: Seamlessly blends the replaced character with the original video environment.
- Smooth Long Video Generation: Iterative generation ensures consistent motion and visual flow in extended videos

## ComfyOrg Wan2.2 Animate stream replay

<iframe
  className="w-full aspect-video rounded-xl"
 src="https://www.youtube.com/embed/5kb-rP0m5BA?si=lbIYkCP5akkG2N6D" 
 title="Wan 2.2 Animate in ComfyUI with Flipping Sigmas / September 19th, 2025" frameborder="0" 
 allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
 referrerpolicy="strict-origin-when-cross-origin" 
 allowfullscreen>
 </iframe>

<UpdateReminder/>


## About Wan2.2 Animate workflow

In this docs, we will provide two workflow:

1. Workflow that only uses core nodes (It is incomplete; you need to preprocess the image by yourself first)
2. Workflow that includes some custom nodes (It is complete; you can use it directly, but some new user might not know how to install the custom nodes)

## Wan2.2 Anmate ComfyUI native workflow(without custom nodes)

### 1. Download Workflow File

Download the following workflow file and drag it into ComfyUI to load the workflow.

<a className="prose"  target='_blank'  href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_animate.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
    <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Download materials below as input:

**Reference Image:**
![Reference_Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_animate/ref_image.png)

**Input Video**
<video
  controls
  className="w-full aspect-video"
  src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_animate/original_video.mp4"
></video>

### 2. Model links

**diffusion_models**       
- [Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors](https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/Wan22Animate/Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors) This is the model that from Kijai's repo
- [wan2.2_animate_14B_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_animate_14B_bf16.safetensors) original model weight

**clip_visions**
- [clip_vision_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)

**loras**
- [lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors) è¿™æ˜¯ä¸€ä¸ª 4 æ­¥çš„åŠ é€Ÿ lora

**vae**
- [wan_2.1_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**text_encoders**   
- [umt5_xxl_fp8_e4m3fn_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
â”œâ”€â”€â”€ðŸ“‚ models/
â”‚   â”œâ”€â”€â”€ðŸ“‚ diffusion_models/
â”‚   â”‚   â”œâ”€â”€â”€ Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors
â”‚   â”‚   â””â”€â”€â”€ wan2.2_animate_14B_bf16.safetensors
â”‚   â”œâ”€â”€â”€ðŸ“‚ loras/
â”‚   â”‚   â””â”€â”€â”€ lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors
â”‚   â”œâ”€â”€â”€ðŸ“‚ text_encoders/
â”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors 
â”‚   â”œâ”€â”€â”€ðŸ“‚ clip_visions/ 
â”‚   â”‚   â””â”€â”€â”€ clip_vision_h.safetensors
â”‚   â””â”€â”€â”€ðŸ“‚ vae/
â”‚       â””â”€â”€ wan_2.1_vae.safetensors
```

### 3. Install custom nodes

Download the following workflow file and drag it into ComfyUI to load the workflow, if you have [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) installed, you can just click the `Install missing nodes` button to install the missing nodes.

We need to install the following custom nodes:

- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)
- [ComfyUI-comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

If you don't know how to install custom nodes please refer to [How to install custom nodes](/installation/install_custom_node)

### 4. Workflow Instructions

The Wan2.2 animate has two modes: Mix and move
- Mix: use the reference image to replace the character in the video
- Move: Use the character movement from the input video to animate the character in the reference image (like Wan2.2 Fun Control)

#### 4.1 Mix mode
![Workflow Instructions](/images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg)

0. If you are running this workflow for the first time, please use a small size for video generation, in case you don't have enough VRAM to run the workflow, and due to the `WanAnimateToVideo` limited, the video width or height should be multiples of 16.
1. Make sure all the models are loaded correctly
2. Update the prompt if you want
3. Upload the reference image, the character is this image will be the target character
4. You can use the videos we provided as input videos for the first timeï¼Œ the **DWPose Estimator** node  in [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) will preprocess the input video to pose and face control videos
5. The `Points Editor` is from [KJNodes](https://github.com/kijai/ComfyUI-KJNodes/), by default this node will not load the first frame from the input video, you need to run the workflow once or manually upload the first frame
    -  Bleow the `Points Editor` node, we have added note about how this node work, and how to edit it please refer to it
6. For the "Video Extend" group, it's in order to extend to the output video length
    - Each `Video Extend` will extend another 77 frames(Around 4.8125 seconds)
    - If your input video is less then 5s, you might not need it
    - If you want to extend longer, you need to copy and paste multiple times, you need to link the `batch_images` from last Video Extend to next one, and also the `video_frame_offset` from last Video Extend to next one
7. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

#### 4.2 Move mode

We used [subgraph](/interface/features/subgraph) in the Wan2.2 animate workflow, here is how to switching to move mode:

![Subgraph](/images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg)

If you want to switch to Move mode, you can disconnect `background_video` and `character_mask` inputs from the `Video Sampling and output(Subgraph)` node.