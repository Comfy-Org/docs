---
title: "ComfyUI Wan-Alpha Video Examples"
description: "This guide demonstrates how to generate videos with alpha channel transparency using Wan-Alpha in ComfyUI"
sidebarTitle: "Wan-Alpha"
---

import UpdateReminder from "/snippets/tutorials/update-reminder.mdx";

## About Wan-Alpha

Wan-Alpha is a specialized text-to-video generation model that produces high-quality videos with alpha channel transparency. Built upon the Wan2.1-14B-T2V base model, it enables the creation of videos with transparent backgrounds and semi-transparent objects.

The model is open-sourced under the [Apache 2.0 license](https://github.com/WeChatCV/Wan-Alpha?tab=Apache-2.0-1-ov-file) and can be used for personal or commercial purposes.

### Key Features

- **Alpha Channel Support**: Generates videos with transparency information, enabling seamless compositing
- **High-Quality Output**: Produces detailed videos with accurate transparency rendering
- **Diverse Capabilities**: Handles various scenes including semi-transparent objects, glowing effects, and fine-grained details like hair
- **Text-to-Video Generation**: Creates videos directly from text prompts with transparency

### Example Results

The model excels at generating:
- Transparent backgrounds for easy compositing
- Semi-transparent objects (bubbles, glass, water)
- Glowing effects and light interactions
- Fine details with proper alpha channels (hair, smoke, particles)

<video controls>
  <source src="https://huggingface.co/htdong/Wan-Alpha/resolve/main/assets/girl.gif" type="video/gif"/>
</video>

### Resources

- [Wan-Alpha GitHub Repository](https://github.com/WeChatCV/Wan-Alpha)
- [Wan-Alpha Model on Hugging Face](https://huggingface.co/htdong/Wan-Alpha)
- [Wan-Alpha Project Page](https://donghaotian123.github.io/Wan-Alpha/)
- [Wan-Alpha ComfyUI Version](https://huggingface.co/htdong/Wan-Alpha_ComfyUI)
- [Research Paper (arXiv:2509.24979)](https://arxiv.org/pdf/2509.24979)
- [YouTube Tutorial](https://www.youtube.com/watch?v=OyDjj7OPUzA)

<UpdateReminder/>

## Model Installation

Wan-Alpha v1.0 was released on September 30, 2025. The model weights and inference code are available on the [GitHub repository](https://github.com/WeChatCV/Wan-Alpha).

For detailed installation and usage instructions, please refer to the [GitHub repository](https://github.com/WeChatCV/Wan-Alpha) as the model continues to be developed and integrated into ComfyUI workflows.

## Technical Details

Wan-Alpha is built on top of the Wan2.1-14B-T2V model and has been specifically adapted to generate alpha channel information alongside RGB video content. This allows for:

- Transparent background generation
- Proper handling of semi-transparent materials
- Accurate edge rendering for compositing
- Support for complex transparency scenarios

## Acknowledgements

Wan-Alpha builds upon several excellent open-source projects:

- [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) - Training/inference framework
- [Wan2.1](https://github.com/Wan-Video/Wan2.1) - Base video generation model
- [LightX2V](https://github.com/ModelTC/LightX2V) - Inference acceleration
- [WanVideo_comfy](https://huggingface.co/Kijai/WanVideo_comfy) - Inference acceleration

## Citation

If you use Wan-Alpha in your research, please cite:

```bibtex
@misc{dong2025wanalpha,
      title={Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel}, 
      author={Haotian Dong and Wenjing Wang and Chen Li and Di Lin},
      year={2025},
      eprint={2509.24979},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.24979}, 
}
```

## Contact

For questions or feedback about Wan-Alpha, please visit the [GitHub Issues](https://github.com/WeChatCV/Wan-Alpha/issues) page.
