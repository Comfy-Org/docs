---
title: "ComfyUI Wan2.2 Fun Inp Start-End Frame Video Generation Example"
description: "This article introduces how to use ComfyUI to complete the Wan2.2 Fun Inp start-end frame video generation example"
sidebarTitle: "Wan2.2 Fun Inp"
---

import UpdateReminder from '/snippets/tutorials/update-reminder.mdx'

**Wan2.2-Fun-Inp** is a start-end frame controlled video generation model launched by Alibaba PAI team. It supports inputting **start and end frame images** to generate intermediate transition videos, providing creators with greater creative control. The model is released under the **Apache 2.0 license** and supports commercial use.

**Key Features**:
- **Start-End Frame Control**: Supports inputting start and end frame images to generate intermediate transition videos, enhancing video coherence and creative freedom
- **High-Quality Video Generation**: Based on the Wan2.2 architecture, outputs film-level quality videos
- **Multi-Resolution Support**: Supports generating videos at 512Ã—512, 768Ã—768, 1024Ã—1024 and other resolutions to suit different scenarios

**Model Version**:
- **14B High-Performance Version**: Model size exceeds 32GB, with better results but requires high VRAM

Below are the relevant model weights and code repositories:

- [ðŸ¤—Wan2.2-Fun-Inp-14B](https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-InP)
- Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

## ComfyOrg Wan2.2 Fun InP & Control Youtube Live Stream Replay

<iframe
  className="w-full aspect-video rounded-xl"
  src="https://www.youtube.com/embed/YcAerNYIvho?si=Zh8tzRwI_OTAFx3m"
  title="ComfyUI Selection Toolbox New Features"
  allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

## Wan2.2 Fun Inp Start-End Frame Video Generation Workflow Example

<UpdateReminder/>

This workflow provides two versions:
1. A version using [Wan2.2-Lightning](https://huggingface.co/lightx2v/Wan2.2-Lightning) 4-step LoRA from lightx2v for accelerated video generation
2. A fp8_scaled version without acceleration LoRA

Below are the test results using an RTX4090D 24GB VRAM GPU at 640Ã—640 resolution with 81 frames

| Model Type               | VRAM Usage | First Generation Time | Second Generation Time |
| ------------------------ | ---------- | -------------------- | --------------------- |
| fp8_scaled               | 83%        | â‰ˆ 524s               | â‰ˆ 520s                |
| fp8_scaled + 4-step LoRA | 89%        | â‰ˆ 138s               | â‰ˆ 79s                 |

Since the acceleration with LoRA is significant but the video dynamic is lost,  the provided workflows enable the accelerated LoRA version by default. If you want to enable the other workflow, select it and use **Ctrl+B** to activate.

### 1. Download Workflow File

Please update your ComfyUI to the latest version, and find "**Wan2.2 Fun Inp**" under the menu `Workflow` -> `Browse Templates` -> `Video` to load the workflow.

Or, after updating ComfyUI to the latest version, download the workflow below and drag it into ComfyUI to load.

<a className="prose"  target='_blank'  href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_animate.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
    <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Use the following materials as the start and end frames

![Wan2.2 Fun Control ComfyUI Workflow Start Frame Material](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_inp/start_image.png)
![Wan2.2 Fun Control ComfyUI Workflow End Frame Material](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_inp/end_image.png)

### 2. Models

**Diffusion Model**
- [wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors)
- [wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors)

**Lightning LoRA (Optional, for acceleration)**
- [wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors)
- [wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors)

**VAE**
- [wan_2.1_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**Text Encoder**   
- [umt5_xxl_fp8_e4m3fn_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
â”œâ”€â”€â”€ðŸ“‚ models/
â”‚   â”œâ”€â”€â”€ðŸ“‚ diffusion_models/
â”‚   â”‚   â”œâ”€â”€â”€ wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors
â”‚   â”‚   â””â”€â”€â”€ wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors
â”‚   â”œâ”€â”€â”€ðŸ“‚ loras/
â”‚   â”‚   â”œâ”€â”€â”€ wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors
â”‚   â”‚   â””â”€â”€â”€ wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors
â”‚   â”œâ”€â”€â”€ðŸ“‚ text_encoders/
â”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors 
â”‚   â””â”€â”€â”€ðŸ“‚ vae/
â”‚       â””â”€â”€ wan_2.1_vae.safetensors
```

### 3. Workflow Guide

![Workflow Step Image](/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg)

<Note>
  This workflow uses LoRA. Please make sure the corresponding Diffusion model and LoRA are matched.
</Note>

1. **High noise** model and **LoRA** loading
   - Ensure the `Load Diffusion Model` node loads the `wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors` model
   - Ensure the `LoraLoaderModelOnly` node loads the `wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors`
2. **Low noise** model and **LoRA** loading
   - Ensure the `Load Diffusion Model` node loads the `wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors` model
   - Ensure the `LoraLoaderModelOnly` node loads the `wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors`
3. Ensure the `Load CLIP` node loads the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
4. Ensure the `Load VAE` node loads the `wan_2.1_vae.safetensors` model
5. Upload the start and end frame images as materials
6. Enter your prompt in the Prompt group
7. Adjust the size and video length in the `WanFunInpaintToVideo` node
    - Adjust the `width` and `height` parameters. The default is `640`. We set a smaller size, but you can modify it as needed.
    - Adjust the `length`, which is the total number of frames. The current workflow fps is 16. For example, if you want to generate a 5-second video, you should set it to 5*16 = 80.
8. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation
