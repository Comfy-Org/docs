---
title: "Qwen-Image ComfyUI Native Workflow Example"
description: "Qwen-Image is a 20B parameter MMDiT (Multimodal Diffusion Transformer) model open-sourced under the Apache 2.0 license."
sidebarTitle: "Qwen-Image"
---

import UpdateReminder from '/snippets/tutorials/update-reminder.mdx'


**Qwen-Image** is the first image generation foundation model released by Alibaba's Qwen team. It's a 20B parameter MMDiT (Multimodal Diffusion Transformer) model open-sourced under the Apache 2.0 license. The model has made significant advances in **complex text rendering** and **precise image editing**, achieving high-fidelity output for multiple languages including English and Chinese.

**Model Highlights**:
- **Excellent Multilingual Text Rendering**: Supports high-precision text generation in multiple languages including English, Chinese, Korean, Japanese, maintaining font details and layout consistency
- **Diverse Artistic Styles**: From photorealistic scenes to impressionist paintings, from anime aesthetics to minimalist design, fluidly adapting to various creative prompts

**Related Links**:
 - [GitHub](https://github.com/QwenLM/Qwen-Image)
 - [Hugging Face](https://huggingface.co/Qwen/Qwen-Image)
 - [ModelScope](https://modelscope.cn/models/qwen/Qwen-Image)

## Qwen-Image Native Workflow Example

<UpdateReminder />



There are three different models used in the workflow attached to this document:
1. Qwen-Image original model fp8_e4m3fn
2. 8-step accelerated version: Qwen-Image original model fp8_e4m3fn with lightx2v 8-step LoRA
3. Distilled version: Qwen-Image distilled model fp8_e4m3fn

**VRAM Usage Reference**
GPU: RTX4090D 24GB

| Model Used                              | VRAM Usage | First Generation | Second Generation |
| --------------------------------------- | ---------- | --------------- | ---------------- |
| fp8_e4m3fn                              | 86%        | â‰ˆ 94s           | â‰ˆ 71s            |
| fp8_e4m3fn with lightx2v 8-step LoRA    | 86%        | â‰ˆ 55s           | â‰ˆ 34s            |
| Distilled fp8_e4m3fn                    | 86%        | â‰ˆ 69s           | â‰ˆ 36s            |


### 1. Workflow File

After updating ComfyUI, you can find the workflow file in the templates, or drag the workflow below into ComfyUI to load it.
![Qwen-image Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/qwen/qwen-image.png)

<a className="prose"  target='_blank'  href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/image_qwen_image.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
    <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Workflow for Qwen-Image Official Model</p>
</a>

Distilled version
<a className="prose"  target='_blank'  href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/qwen/image_qwen_image_distill.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
    <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Workflow for Distilled Model</p>
</a>

### 2. Model Download

**Available Models in ComfyUI**

- Qwen-Image_bf16 (40.9 GB)
- Qwen-Image_fp8 (20.4 GB)
- Distilled versions (non-official, requires only 15 steps)

All models are available at [Huggingface](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/tree/main) and [Modelscope](https://modelscope.cn/models/Comfy-Org/Qwen-Image_ComfyUI/files)

**Diffusion model**

- [qwen_image_fp8_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_fp8_e4m3fn.safetensors)

Qwen_image_distill

- [qwen_image_distill_full_fp8_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/non_official/diffusion_models/qwen_image_distill_full_fp8_e4m3fn.safetensors)
- [qwen_image_distill_full_bf16.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/non_official/diffusion_models/qwen_image_distill_full_bf16.safetensors)

<Note>
- The original author of the distilled version recommends using 15 steps with cfg 1.0.
- According to tests, this distilled version also performs well at 10 steps with cfg 1.0. You can choose either euler or res_multistep based on the type of image you want.
</Note>

**LoRA**

- [Qwen-Image-Lightning-8steps-V1.0.safetensors](https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Lightning-8steps-V1.0.safetensors)

**Text encoder**

- [qwen_2.5_vl_7b_fp8_scaled.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors)

**VAE**

[qwen_image_vae.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/vae/qwen_image_vae.safetensors)

**Model Storage Location**

```
ğŸ“‚ ComfyUI/
â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”œâ”€â”€ ğŸ“‚ diffusion_models/
â”‚   â”‚   â”œâ”€â”€ qwen_image_fp8_e4m3fn.safetensors
â”‚   â”‚   â””â”€â”€ qwen_image_distill_full_fp8_e4m3fn.safetensors ## è’¸é¦ç‰ˆ
â”‚   â”œâ”€â”€ ğŸ“‚ loras/
â”‚   â”‚   â””â”€â”€ Qwen-Image-Lightning-8steps-V1.0.safetensors   ## 8æ­¥åŠ é€Ÿ LoRA æ¨¡å‹
â”‚   â”œâ”€â”€ ğŸ“‚ vae/
â”‚   â”‚   â””â”€â”€ qwen_image_vae.safetensors
â”‚   â””â”€â”€ ğŸ“‚ text_encoders/
â”‚       â””â”€â”€ qwen_2.5_vl_7b_fp8_scaled.safetensors
```

### 3. Complete the Workflow Step by Step

![Step Guide](/images/tutorial/image/qwen/image_qwen_image-guide.jpg)

1. Make sure the `Load Diffusion Model` node has loaded `qwen_image_fp8_e4m3fn.safetensors`
2. Make sure the `Load CLIP` node has loaded `qwen_2.5_vl_7b_fp8_scaled.safetensors`
3. Make sure the `Load VAE` node has loaded `qwen_image_vae.safetensors`
4. Make sure the `EmptySD3LatentImage` node is set with the correct image dimensions
5. Set your prompt in the `CLIP Text Encoder` node; currently, it supports at least English, Chinese, Korean, Japanese, Italian, etc.
6. If you want to enable the 8-step acceleration LoRA by lightx2v, select the node and use `Ctrl + B` to enable it, and modify the Ksampler settings as described in step 8
7. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow
8. For different model versions and workflows, adjust the KSampler parameters accordingly

<Note>
 The distilled model and the 8-step acceleration LoRA by lightx2v do not seem to be compatible for simultaneous use. You can experiment with different combinations to verify if they can be used together.
</Note>