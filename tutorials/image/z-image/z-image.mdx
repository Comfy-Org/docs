---
title: "Z-Image ComfyUI Workflow Example"
description: "Z-Image is a 6B parameter efficient image generation foundation model with single-stream diffusion transformer, offering sub-second inference latency."
sidebarTitle: "Z-Image"
---

import UpdateReminder from '/snippets/tutorials/update-reminder.mdx'

**Z-Image (ÈÄ†Áõ∏)** is a powerful and highly efficient image generation model with **6B** parameters, developed by Alibaba's Tongyi Lab. It uses a **Scalable Single-Stream DiT** (S3-DiT) architecture where text, visual semantic tokens, and image VAE tokens are concatenated at the sequence level to serve as a unified input stream, maximizing parameter efficiency.

**Model Variants**:
- üöÄ **Z-Image-Turbo** ‚Äì A distilled version that matches or exceeds leading competitors with only **8 NFEs** (Number of Function Evaluations). It offers **sub-second inference latency** on enterprise-grade H800 GPUs and fits within **16GB VRAM consumer devices**.
- üß± **Z-Image-Base** ‚Äì The non-distilled foundation model for community-driven fine-tuning and custom development.
- ‚úçÔ∏è **Z-Image-Edit** ‚Äì A variant fine-tuned for image editing tasks with impressive instruction-following capabilities.

**Model Highlights**:
- **Photorealistic Quality**: Delivers strong photorealistic image generation while maintaining excellent aesthetic quality
- **Accurate Bilingual Text Rendering**: Excels at accurately rendering complex Chinese and English text
- **Prompt Enhancing & Reasoning**: Prompt Enhancer empowers the model with reasoning capabilities
- **Sub-second Inference**: Achieves fast generation speed on supported hardware

**Related Links**:
- [GitHub](https://github.com/Tongyi-MAI/Z-Image)
- [Hugging Face](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)
- [ModelScope](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)
- [Online Demo (HuggingFace)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)
- [Online Demo (ModelScope)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO)

## Z-Image-Turbo text-to-image workflow

<UpdateReminder />

### 1. Install diffusers from source

Z-Image support has been merged into the latest diffusers release. You need to install diffusers from source:

```bash
pip install git+https://github.com/huggingface/diffusers
```

### 2. Model download

Download the model from Hugging Face:

```bash
pip install -U huggingface_hub
HF_XET_HIGH_PERFORMANCE=1 hf download Tongyi-MAI/Z-Image-Turbo
```

Or download from ModelScope for users in China.

### 3. Quick start code

```python
import torch
from diffusers import ZImagePipeline

# 1. Load the pipeline
# Use bfloat16 for optimal performance on supported GPUs
pipe = ZImagePipeline.from_pretrained(
    "Tongyi-MAI/Z-Image-Turbo",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=False,
)
pipe.to("cuda")

# [Optional] Attention Backend
# Diffusers uses SDPA by default. Switch to Flash Attention for better efficiency if supported:
# pipe.transformer.set_attention_backend("flash")    # Enable Flash-Attention-2
# pipe.transformer.set_attention_backend("_flash_3") # Enable Flash-Attention-3

# [Optional] Model Compilation
# Compiling the DiT model accelerates inference, but the first run will take longer to compile.
# pipe.transformer.compile()

# [Optional] CPU Offloading
# Enable CPU offloading for memory-constrained devices.
# pipe.enable_model_cpu_offload()

prompt = "Young Chinese woman in red Hanfu, intricate embroidery. Impeccable makeup, red floral forehead pattern. Elaborate high bun, golden phoenix headdress, red flowers, beads. Holds round folding fan with lady, trees, bird. Neon lightning-bolt lamp (‚ö°Ô∏è), bright yellow glow, above extended left palm. Soft-lit outdoor night background, silhouetted tiered pagoda (Ë•øÂÆâÂ§ßÈõÅÂ°î), blurred colorful distant lights."

# 2. Generate Image
image = pipe(
    prompt=prompt,
    height=1024,
    width=1024,
    num_inference_steps=9,  # This actually results in 8 DiT forwards
    guidance_scale=0.0,     # Guidance should be 0 for the Turbo models
    generator=torch.Generator("cuda").manual_seed(42),
).images[0]

image.save("example.png")
```

### 4. Key parameters

| Parameter | Recommended Value | Description |
| --------- | ----------------- | ----------- |
| `num_inference_steps` | 9 | Results in 8 DiT forwards |
| `guidance_scale` | 0.0 | Should be 0 for Turbo models |
| `torch_dtype` | `torch.bfloat16` | Optimal performance on supported GPUs |

### 5. Performance optimization

**Attention Backend**

Switch to Flash Attention for better efficiency:
```python
pipe.transformer.set_attention_backend("flash")    # Flash-Attention-2
pipe.transformer.set_attention_backend("_flash_3") # Flash-Attention-3
```

**Model Compilation**

Compile the DiT model for faster inference (first run will be slower):
```python
pipe.transformer.compile()
```

**CPU Offloading**

For memory-constrained devices:
```python
pipe.enable_model_cpu_offload()
```

## Citation

If you find Z-Image useful in your research, please consider citing:

```bibtex
@misc{z-image-2025,
  title={Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer},
  author={Tongyi Lab},
  year={2025},
  publisher={GitHub},
  journal={GitHub repository},
  howpublished={\url{https://github.com/Tongyi-MAI/Z-Image}}
}
```
