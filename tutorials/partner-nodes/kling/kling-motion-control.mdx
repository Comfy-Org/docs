---
title: "Kling 2.6 Motion Control API Node ComfyUI Official Example"
description: "Learn how to use the Kling 2.6 Motion Control Partner node in ComfyUI for precise motion transfer from reference videos to character images"
sidebarTitle: "Kling 2.6 Motion Control"
---

import ReqHint from "/snippets/tutorials/partner-nodes/req-hint.mdx";
import UpdateReminder from "/snippets/tutorials/update-reminder.mdx";

Kling 2.6 Motion Control is a specialized multimodal model that enables precise motion transfer from reference videos to character images. By combining a **Reference Image** (your character) and a **Motion Reference Video** (the action), the AI applies the movement, expression, and pacing of the video to your static character while maintaining their identity.

## Product highlights

- **Complex motion handling**: Execute complicated sequences like dance routines or martial arts without losing character coherence
- **Precision hand and finger performance**: Improved finger articulation and hand movements by mimicking real footage
- **Scene and environment flexibility**: Use text prompts to change the environment while the character continues their referenced motion
- **Advanced camera and perspective modes**: Granular control over how the camera interprets your reference with distinct orientation modes

<ReqHint/>
<UpdateReminder/>

## Kling 2.6 Motion Control workflow

### 1. Download the workflow file

<a className="prose" target='_blank' href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/api_kling_motion_control.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
    <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download the workflow file in JSON format</p>
</a>

### 2. Follow the steps to run the workflow

1. Load your **Reference Image** in the `Load Image` node - this is the character you want to animate
   - Ensure the character's limbs are visible and not obstructed
   - Leave breathing room around the subject for movement
2. Load your **Motion Reference Video** in the `Load Video` node - this provides the movement to transfer
   - Choose videos with a clear subject and clean background
   - Match the framing alignment with your reference image
3. (Optional) Enter a text prompt to guide the environment and style
4. Select the model tier:
   - **Standard**: Best for simple animations, memes, and quick social media clips
   - **Pro**: For complex choreography, intricate hand movements, and professional marketing assets
5. Set the `character_orientation` parameter:
   - `video`: Output character's orientation matches the reference video - better for complex motions (max 30s)
   - `image`: Output character's orientation matches the reference image - better for following camera movements (max 10s)
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to start video generation
7. After the API returns the result, view the generated video in the `Save Video` node. The video will also be saved in the `ComfyUI/output/` directory

## Tips for better results

- **Match aspect ratios**: Ensure your character image and reference video have similar aspect ratios to prevent awkward stretching or cropping
- **Clean backgrounds**: Use reference videos with simple or static backgrounds for best motion extraction
- **Clear character angles**: If your reference video shows rotation, use 3D-style characters or realistic photos that handle rotation better
- **Visible limbs**: If the motion requires hand waving but the character has hands in pockets, the AI may produce artifacts
