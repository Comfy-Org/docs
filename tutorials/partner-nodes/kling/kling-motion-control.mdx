---
title: "Kling 2.6 Motion Control API Node ComfyUI Official Example"
description: "Learn how to use the Kling 2.6 Motion Control Partner node in ComfyUI for precise motion transfer from reference videos to character images"
sidebarTitle: "Kling 2.6 Motion Control"
---

import ReqHint from "/snippets/tutorials/partner-nodes/req-hint.mdx";
import UpdateReminder from "/snippets/tutorials/update-reminder.mdx";

Kling 2.6 Motion Control is a specialized multimodal model developed by Kuaishou that enables precise motion transfer from reference videos to character images. By combining a **Reference Image** (your character) and a **Motion Reference Video** (the action), the AI applies the movement, expression, and pacing of the video to your static character while maintaining their identity.

Unlike standard image-to-video generation that guesses motion from text prompts, Motion Control uses a reference video as the movement blueprint. The model acts as a digital puppeteer, extracting choreography from your reference video and applying it to your character with cinema-grade fidelity.

## Product highlights

- **Complex motion handling**: Execute complicated sequences like dance routines, martial arts, or athletic movements without losing character coherence. The model understands weight transfer and momentum for realistic physical impact.
- **Precision hand and finger performance**: Hands have traditionally been a weak point for AI video. This feature specifically improves finger articulation and hand movements by mimicking real footage, making it ideal for presentations and demonstrations.
- **Scene and environment flexibility**: Use text prompts to change the environment while the character continues their referenced motion. You're not limited to the reference video's background.
- **Advanced camera and perspective modes**: Granular control over how the camera interprets your reference with distinct orientation modes.
- **30-second one-shot support**: Generate continuous actions up to 30 seconds in a single generation, enabling uninterrupted character motion for narrative scenes.

## Character orientation modes

The `character_orientation` parameter determines how the model interprets spatial information and constrains output duration:

| Mode | Description | Max Duration |
|------|-------------|--------------|
| `video` | Output character orientation matches the reference video. Best for complex full-body performances like dance sequences and elaborate choreography. | 30 seconds |
| `image` | Output character orientation matches the reference image. Best for portrait animations with camera movement like pans, tilts, and tracking shots. | 10 seconds |

## Model tiers

| Tier | Resolution | Best For |
|------|------------|----------|
| **Standard** | 720p | Simple animations, social media content, memes, and quick tests. Faster and more credit-efficient. |
| **Pro** | 1080p | Complex choreography, intricate hand movements, professional marketing assets, and broadcast-quality content. |

<ReqHint/>
<UpdateReminder/>

## Kling 2.6 Motion Control workflow

<a className="prose" target='_blank' href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/api_kling_motion_control.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
    <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download the workflow file in JSON format</p>
</a>

## Input requirements

- **Image formats**: JPG, PNG, WEBP, GIF, AVIF (max 10MB)
- **Video formats**: MP4, MOV, WEBM, M4V, GIF (max 100MB)
- **Video duration**: 3-30 seconds depending on orientation mode
- **Minimum resolution**: 720px width and height
- **Subject visibility**: Character must clearly show head, shoulders, and torso

## Tips for better results

- **Match aspect ratios**: Ensure your character image and reference video have similar aspect ratios (e.g., both 16:9 or both 9:16) to prevent awkward stretching or cropping.
- **Clean backgrounds**: Use reference videos with simple or static backgrounds for best motion extraction. High-contrast videos where the actor's silhouette is distinct work best.
- **Clear character angles**: If your reference video shows rotation, use 3D-style characters or realistic photos that handle rotation better. Flat 2D cartoons may struggle with back views.
- **Visible limbs**: Ensure the character's limbs are visible in the source image. If a character has hands in pockets but the motion requires waving, the AI will hallucinate hands, often leading to artifacts.
- **Leave breathing room**: Leave negative space around the subject. If the character will dance or move arms wide, they need space within the frame to avoid clipping.
- **Framing alignment**: Match the framing between your image and reference video. Use a close-up reference for face animations, or a full-body reference for walking/dancing sequences.
