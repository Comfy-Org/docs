---
title: "ComfyUI Flux Kontext Native Workflow Example"
description: "This article briefly introduces the Flux Kontext native workflow example."
sidebarTitle: "Flux Kontext"
---

import PromptTechniques from "/snippets/tutorials/flux/prompt-techniques.mdx";

## About FLUX.1 Kontext

FLUX.1 Kontext is a breakthrough multimodal image editing model developed by Black Forest Labs, marking a significant technological leap from traditional "text-to-image" to "context-aware image generation." Unlike existing models, FLUX.1 Kontext supports simultaneous text and image input, intelligently understanding image context and performing precise editing operations.
The core advantage of FLUX.1 Kontext lies in its excellent context understanding ability and character consistency maintenance, ensuring that key elements such as character features and composition layout remain stable even after multiple iterative edits.

### Version Information

- **[FLUX.1 Kontext [pro]** - Commercial version, focused on rapid iterative editing
- **FLUX.1 Kontext [max]** - Experimental version with stronger prompt adherence
- **FLUX.1 Kontext [dev]** - Open source version (used in this tutorial), 12B parameters, mainly for research

Currently in ComfyUI, you can use all these versions, where [Pro and Max versions](tutorials/api-nodes/black-forest-labs/flux-1-kontext) can be called through API nodes, while the Dev open source version please refer to the instructions in this guide.

### Core Capabilities

- **Character Consistency**: Maintains character features stable after multiple edits
- **Local Editing**: Precisely modifies specific areas of the image
- **Iterative Editing**: Supports gradual optimization and multiple rounds of modifications
- **High-Speed Inference**: Up to 8x faster than similar models

## ComfyUI Flux Kontext Native Workflow

### 1. Workflow and Input Image Download

The `metadata` of the following image contains workflow and model download information, please download and drag it into ComfyUI to load the corresponding workflow

![ComfyUI Flux Kontext Native Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/flux1_kontext.png)
Please download the following image as input

![ComfyUI Flux Kontext Native Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/input.png)

### 2. Manual Model Download

If you cannot complete the model download successfully, please manually download the following files

**Diffusion Model**

[Add links]

<Tip>
You may already have these models below
</Tip>

**VAE**
- [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/blob/main/split_files/vae/ae.safetensors)

**Text Encoder**
- [clip_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors)
- [t5xxl_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors) or [t5xxl_fp8_e4m3fn_scaled.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors)

Model save location

```
ðŸ“‚ ComfyUI/
â”œâ”€â”€ ðŸ“‚ models/
â”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/
â”‚   â”‚   â””â”€â”€ [to be updated]
â”‚   â”œâ”€â”€ ðŸ“‚ vae/
â”‚   â”‚   â””â”€â”€ ae.safetensor
â”‚   â””â”€â”€ ðŸ“‚ text_encoders/
â”‚       â”œâ”€â”€ clip_l.safetensors
â”‚       â””â”€â”€ t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn_scaled.safetensors
```

### 3. Complete the workflow step by step

![ComfyUI Flux Kontext Native Workflow Step Guide](/images/tutorial/flux/flux_1_kontext_step_guide.jpg)

You can refer to the numbers in the picture to complete the workflow run:

<Note>
    In our actual tests, when using the original model, there is a situation where the VRAM is insufficient on a 4090 with 24GB VRAM 1024x1024, while it can run smoothly on an A100 with 40GB VRAM 1024x1024.
    In this workflow, we default to using the `fp8_e4m3fn` precision setting to ensure that it can also run on ordinary consumer-grade GPUs. If you need to try the original precision, please modify it to `default`
</Note>

1. In the `Load Diffusion Model` node, load the `flux_1_kontext_pro_image.safetensors` model, note that the `weight_dtype` has been set to `fp8_e4m3fn` to ensure that you can also run on ordinary consumer-grade GPUs. If you need to try the original precision, please modify it to `default`
2. In the `DualCLIP Load` node, ensure that `clip_l.safetensors` and `t5xxl_fp16.safetensors` or `t5xxl_fp8_e4m3fn_scaled.safetensors` are loaded
3. In the `Load VAE` node, ensure that `ae.safetensors` is loaded
4. In the `Image size` node, set the size
5. In the `Load Image` node, load the image to be edited
6. In the `CLIP Text Encode` node, modify the prompt, currently only English is supported

<PromptTechniques/>