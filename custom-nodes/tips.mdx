---
title: "Tips"
---

## Recommended Development Lifecycle

### Project Setup

1. **Create a dedicated development environment**
   ```bash
   # Create a virtual environment for your custom node
   python -m venv comfyui_dev
   source comfyui_dev/bin/activate  # On Windows: comfyui_dev\Scripts\activate
   
   # Install ComfyUI in development mode
   git clone https://github.com/comfyanonymous/ComfyUI.git
   cd ComfyUI
   pip install -e .
   ```

2. **Set up your custom node directory structure**
   ```
   custom_nodes/
   └── your_node_pack/
       ├── __init__.py
       ├── nodes.py
       ├── requirements.txt
       ├── README.md
       └── locales/
           ├── en/
           │   └── main.json
           └── zh/
               └── main.json
   ```

3. **Enable hot reloading for faster development**
   - Use `--auto-launch` flag when starting ComfyUI
   - Implement proper error handling to avoid crashes during development
   - Use logging instead of print statements for debugging

### Development Workflow

1. **Start with a minimal working example**
   ```python
   class MyFirstNode:
       @classmethod
       def INPUT_TYPES(s):
           return {"required": {"text": ("STRING", {"default": "Hello World"})}}
       
       RETURN_TYPES = ("STRING",)
       FUNCTION = "process"
       CATEGORY = "custom"
       
       def process(self, text):
           return (f"Processed: {text}",)
   ```

2. **Iterative development approach**
   - Implement core functionality first
   - Add input validation and error handling
   - Optimize performance and memory usage
   - Add advanced features and customization options

3. **Testing strategy**
   - Test with various input types and edge cases
   - Verify memory usage with large batches
   - Test integration with other nodes
   - Validate output formats and data types

## Debugging Techniques

### Python Backend Debugging

1. **Logging setup**
   ```python
   import logging
   
   # Set up logging for your custom node
   logger = logging.getLogger(__name__)
   logger.setLevel(logging.DEBUG)
   
   # In your node methods
   def process(self, input_data):
       logger.debug(f"Processing input: {type(input_data)}, shape: {getattr(input_data, 'shape', 'N/A')}")
       try:
           result = self.do_processing(input_data)
           logger.info(f"Processing completed successfully")
           return result
       except Exception as e:
           logger.error(f"Processing failed: {str(e)}")
           raise
   ```

2. **Memory debugging**
   ```python
   import torch
   import psutil
   import os
   
   def log_memory_usage(stage=""):
       if torch.cuda.is_available():
           gpu_memory = torch.cuda.memory_allocated() / 1024**3
           print(f"[{stage}] GPU Memory: {gpu_memory:.2f} GB")
       
       process = psutil.Process(os.getpid())
       cpu_memory = process.memory_info().rss / 1024**3
       print(f"[{stage}] CPU Memory: {cpu_memory:.2f} GB")
   ```

3. **Tensor debugging utilities**
   ```python
   def debug_tensor(tensor, name="tensor"):
       if tensor is None:
           print(f"{name}: None")
           return
       
       print(f"{name}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}")
       print(f"  min={tensor.min().item():.4f}, max={tensor.max().item():.4f}, mean={tensor.mean().item():.4f}")
       
       if torch.isnan(tensor).any():
           print(f"  WARNING: {name} contains NaN values!")
       if torch.isinf(tensor).any():
           print(f"  WARNING: {name} contains infinite values!")
   ```

### JavaScript Frontend Debugging

1. **Console debugging**
   ```javascript
   // Enable detailed logging
   app.registerExtension({
       name: "MyNode.Debug",
       setup() {
           console.log("MyNode extension loaded");
       },
       nodeCreated(node) {
           if (node.comfyClass === "MyCustomNode") {
               console.log("MyCustomNode created:", node);
               
               // Log widget changes
               if (node.widgets) {
                   node.widgets.forEach(widget => {
                       const originalCallback = widget.callback;
                       widget.callback = function(value) {
                           console.log(`Widget ${widget.name} changed to:`, value);
                           return originalCallback?.apply(this, arguments);
                       };
                   });
               }
           }
       }
   });
   ```

2. **Network debugging**
   ```javascript
   // Monitor API calls
   const originalFetch = window.fetch;
   window.fetch = function(...args) {
       console.log("API call:", args[0]);
       return originalFetch.apply(this, args)
           .then(response => {
               console.log("API response:", response.status, response.statusText);
               return response;
           });
   };
   ```

3. **Graph debugging**
   ```javascript
   // Monitor graph changes
   app.graph.onAfterChange = function() {
       console.log("Graph changed, nodes:", this.nodes.length);
       console.log("Links:", this.links.length);
   };
   ```

## Performance Optimization

### Memory Management

1. **Efficient tensor operations**
   ```python
   # Use in-place operations when possible
   def process_inplace(self, tensor):
       tensor.mul_(0.5)  # In-place multiplication
       tensor.add_(0.1)  # In-place addition
       return tensor
   
   # Clear intermediate tensors
   def process_with_cleanup(self, input_tensor):
       intermediate = input_tensor * 2.0
       result = torch.nn.functional.relu(intermediate)
       del intermediate  # Explicit cleanup
       return result
   ```

2. **Batch processing optimization**
   ```python
   def process_batch(self, images):
       batch_size = images.shape[0]
       
       # Process in smaller chunks to avoid OOM
       chunk_size = min(4, batch_size)
       results = []
       
       for i in range(0, batch_size, chunk_size):
           chunk = images[i:i+chunk_size]
           chunk_result = self.process_chunk(chunk)
           results.append(chunk_result)
           
           # Clear GPU cache periodically
           if torch.cuda.is_available():
               torch.cuda.empty_cache()
       
       return torch.cat(results, dim=0)
   ```

### Frontend Performance

1. **Efficient DOM operations**
   ```javascript
   // Batch DOM updates
   function updateNodeDisplay(node, updates) {
       const fragment = document.createDocumentFragment();
       
       updates.forEach(update => {
           const element = document.createElement(update.type);
           element.textContent = update.content;
           fragment.appendChild(element);
       });
       
       node.domElement.appendChild(fragment);
   }
   ```

2. **Debounced updates**
   ```javascript
   function debounce(func, wait) {
       let timeout;
       return function executedFunction(...args) {
           const later = () => {
               clearTimeout(timeout);
               func(...args);
           };
           clearTimeout(timeout);
           timeout = setTimeout(later, wait);
       };
   }
   
   const debouncedUpdate = debounce(updateNodeState, 100);
   ```

## Best Practices

### Code Organization

1. **Modular design**
   ```python
   # Separate concerns into different modules
   from .core import BaseProcessor
   from .utils import validate_input, format_output
   from .constants import DEFAULT_SETTINGS
   
   class MyNode(BaseProcessor):
       def __init__(self):
           super().__init__()
           self.settings = DEFAULT_SETTINGS.copy()
       
       def process(self, input_data):
           validated_input = validate_input(input_data)
           result = self.core_processing(validated_input)
           return format_output(result)
   ```

2. **Configuration management**
   ```python
   import json
   from pathlib import Path
   
   class NodeConfig:
       def __init__(self, config_path="config.json"):
           self.config_path = Path(config_path)
           self.config = self.load_config()
       
       def load_config(self):
           if self.config_path.exists():
               with open(self.config_path, 'r') as f:
                   return json.load(f)
           return self.get_default_config()
       
       def save_config(self):
           with open(self.config_path, 'w') as f:
               json.dump(self.config, f, indent=2)
   ```

### Error Handling

1. **Graceful error handling**
   ```python
   def safe_process(self, input_data):
       try:
           return self.process(input_data)
       except ValueError as e:
           raise ValueError(f"Invalid input data: {str(e)}")
       except RuntimeError as e:
           raise RuntimeError(f"Processing failed: {str(e)}")
       except Exception as e:
           raise Exception(f"Unexpected error in {self.__class__.__name__}: {str(e)}")
   ```

2. **Input validation**
   ```python
   def validate_inputs(self, **kwargs):
       for key, value in kwargs.items():
           if value is None:
               raise ValueError(f"Required input '{key}' is None")
           
           if isinstance(value, torch.Tensor):
               if value.numel() == 0:
                   raise ValueError(f"Input tensor '{key}' is empty")
               if torch.isnan(value).any():
                   raise ValueError(f"Input tensor '{key}' contains NaN values")
   ```

### User Experience

1. **Informative node descriptions**
   ```python
   class MyNode:
       """
       A comprehensive image processing node that applies advanced filters.
       
       This node supports batch processing and provides multiple filter options
       for enhanced image quality. Optimized for both CPU and GPU processing.
       """
       
       @classmethod
       def INPUT_TYPES(s):
           return {
               "required": {
                   "image": ("IMAGE", {"tooltip": "Input image to process"}),
                   "strength": ("FLOAT", {
                       "default": 1.0, 
                       "min": 0.0, 
                       "max": 2.0, 
                       "step": 0.1,
                       "tooltip": "Processing strength (0.0 = no effect, 2.0 = maximum effect)"
                   }),
               }
           }
   ```

2. **Progress indicators**
   ```python
   from tqdm import tqdm
   
   def process_with_progress(self, batch_data):
       results = []
       
       with tqdm(total=len(batch_data), desc="Processing") as pbar:
           for item in batch_data:
               result = self.process_single(item)
               results.append(result)
               pbar.update(1)
       
       return results
   ```

## Testing Strategies

### Unit Testing

```python
import unittest
import torch
from your_node import MyCustomNode

class TestMyCustomNode(unittest.TestCase):
    def setUp(self):
        self.node = MyCustomNode()
    
    def test_basic_functionality(self):
        # Test with standard input
        input_tensor = torch.randn(1, 3, 256, 256)
        result = self.node.process(input_tensor)
        
        self.assertIsInstance(result, tuple)
        self.assertEqual(len(result), 1)
        self.assertIsInstance(result[0], torch.Tensor)
    
    def test_edge_cases(self):
        # Test with empty tensor
        with self.assertRaises(ValueError):
            self.node.process(torch.empty(0))
        
        # Test with invalid dimensions
        with self.assertRaises(ValueError):
            self.node.process(torch.randn(256, 256))  # Missing batch dimension

if __name__ == '__main__':
    unittest.main()
```

### Integration Testing

```python
def test_node_integration():
    """Test node integration with ComfyUI workflow"""
    from nodes import LoadImage, SaveImage
    
    # Create a simple workflow
    load_node = LoadImage()
    my_node = MyCustomNode()
    save_node = SaveImage()
    
    # Load test image
    image_path = "test_images/sample.png"
    loaded_image = load_node.load_image(image_path)
    
    # Process with custom node
    processed = my_node.process(loaded_image[0])
    
    # Save result
    save_result = save_node.save_images(processed[0])
    
    assert save_result is not None
    print("Integration test passed!")
```

## Common Pitfalls

### Python Development

1. **Memory leaks**
   ```python
   # BAD: Accumulating references
   class BadNode:
       def __init__(self):
           self.cache = []  # This will grow indefinitely
       
       def process(self, data):
           self.cache.append(data)  # Memory leak!
           return self.do_work(data)
   
   # GOOD: Proper memory management
   class GoodNode:
       def __init__(self):
           self.cache = {}
           self.max_cache_size = 100
       
       def process(self, data):
           if len(self.cache) > self.max_cache_size:
               self.cache.clear()
           
           cache_key = self.get_cache_key(data)
           if cache_key not in self.cache:
               self.cache[cache_key] = self.do_work(data)
           
           return self.cache[cache_key]
   ```

2. **Incorrect tensor operations**
   ```python
   # BAD: Modifying input tensors
   def bad_process(self, input_tensor):
       input_tensor += 1.0  # Modifies original tensor!
       return input_tensor
   
   # GOOD: Creating new tensors
   def good_process(self, input_tensor):
       result = input_tensor + 1.0  # Creates new tensor
       return result
   ```

### JavaScript Development

1. **Memory leaks in event listeners**
   ```javascript
   // BAD: Not removing event listeners
   nodeCreated(node) {
       node.addEventListener('click', this.handleClick);
   }
   
   // GOOD: Proper cleanup
   nodeCreated(node) {
       const handleClick = () => { /* handler */ };
       node.addEventListener('click', handleClick);
       
       // Store reference for cleanup
       node._customClickHandler = handleClick;
   }
   
   nodeRemoved(node) {
       if (node._customClickHandler) {
           node.removeEventListener('click', node._customClickHandler);
           delete node._customClickHandler;
       }
   }
   ```

2. **Blocking the UI thread**
   ```javascript
   // BAD: Synchronous heavy operations
   function badProcessing(data) {
       for (let i = 0; i < 1000000; i++) {
           // Heavy computation blocks UI
           processItem(data[i]);
       }
   }
   
   // GOOD: Asynchronous processing
   async function goodProcessing(data) {
       const chunkSize = 1000;
       for (let i = 0; i < data.length; i += chunkSize) {
           const chunk = data.slice(i, i + chunkSize);
           await new Promise(resolve => {
               setTimeout(() => {
                   chunk.forEach(processItem);
                   resolve();
               }, 0);
           });
       }
   }
   ```

## Development Environment Setup

### Recommended Tools

1. **IDE Configuration**
   - **VS Code**: Install Python and JavaScript extensions
   - **PyCharm**: Configure ComfyUI as a Python project
   - **Debugging**: Set up remote debugging for ComfyUI server

2. **Version Control**
   ```bash
   # Initialize git repository for your custom node
   git init
   git add .gitignore README.md
   git commit -m "Initial commit"
   
   # Create development branch
   git checkout -b development
   ```

3. **Automated Testing**
   ```bash
   # Set up pre-commit hooks
   pip install pre-commit
   pre-commit install
   
   # Create test configuration
   echo "python -m pytest tests/" > test.sh
   chmod +x test.sh
   ```

### Hot Reloading Setup

```python
# Add to your __init__.py for development
import importlib
import sys

def reload_modules():
    """Reload all modules in this package for development"""
    if __name__ in sys.modules:
        for module_name in list(sys.modules.keys()):
            if module_name.startswith(__name__):
                importlib.reload(sys.modules[module_name])

# Call during development
if os.getenv('COMFYUI_DEV_MODE'):
    reload_modules()
```
