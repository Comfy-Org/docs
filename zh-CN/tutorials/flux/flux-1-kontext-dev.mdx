---
title: "ComfyUI Flux Kontext Dev 原生工作流示例"
description: "ComfyUI Flux Kontext Dev 原生工作流示例。"
sidebarTitle: "Flux Kontext Dev"
---

import PromptTechniques from "/snippets/zh/tutorials/flux/prompt-techniques.mdx";

## 关于 FLUX.1 Kontext Dev

FLUX.1 Kontext 是 Black Forest Labs 推出的突破性多模态图像编辑模型，支持文本和图像同时输入，能够智能理解图像上下文并执行精确编辑。其开发版是一个拥有 120 亿参数的开源扩散变压器模型，具有出色的上下文理解能力和角色一致性保持，即使经过多次迭代编辑，也能确保人物特征、构图布局等关键元素保持稳定。

与 FLUX.1 Kontext 套件具备相同的核心能力：
角色一致性：在多个场景和环境中保留图像的独特元素，例如图片中的参考角色或物体。
局部编辑：对图像中的特定元素进行有针对性的修改，而不影响其他部分。
风格参考：根据文本提示，在保留参考图像独特风格的同时生成新颖场景。
交互速度：图像生成和编辑的延迟极小。

虽然之前发布的 API 版本提供了最高的保真度和速度，但 FLUX.1 Kontext [Dev] 完全在本地机器上运行，为希望进行实验的开发者、研究人员和高级用户提供了无与伦比的灵活性。

### 版本说明

- **[FLUX.1 Kontext [pro]** - 商业版本，专注快速迭代编辑
- **FLUX.1 Kontext [max]** - 实验版本，更强的提示遵循能力  
- **FLUX.1 Kontext [dev]** - 开源版本（本教程使用），12B参数，主要用于研究

目前在 ComfyUI 中，你可以使用所有的这些版本，其中 [Pro 及 Max 版本](zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-kontext) 可以通过 API 节点来进行调用，而 Dev 版本开源版本请参考本篇指南中的说明。


## 工作流说明

目前在本篇教程中，我们涉及了两类工作流，本质上他们其实是相同的，
- 使用了组节点 **FLUX.1 Kontext Image Edit** 的工作流，使得整个界面和工作流复用起来变得简单
- 而另一个工作流没有使用组节点，是完整的原始工作流。

使用组节点的主要优点是工作流简洁，你可以复用组节点来实现复杂的工作流，快速复用节点组，另外在新版本的前端中，我们也为 Flux.1 Kontext Dev 增加了一个快速添加组节点的功能：

![快速添加组节点](/images/tutorial/flux/selcetion_toolbox_edit.jpg)

<Tip>
    这个功能目前只是一个实验性的新功能，可能在未来版本中进行调整。
</Tip>

## ComfyUI Flux Kontext 原生工作流

### 1. 工作流及输入图片下载

下面的图片的`metadata`中已经包含工作流及模型下载信息，请下载并拖入 ComfyUI 中加载对应工作流

![ComfyUI Flux Kontext 原生工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/flux1_kontext.png)
请下载下面的图片作为输入

![ComfyUI Flux Kontext 原生工作流输入](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/input.png)

### 2. 手动模型下载

如果你无法顺利完成模型下载，请手动下载下面的文件

**Diffusion Model**

【补充链接】

<Tip>
下面的这些模型你可能已经有了
</Tip>

**VAE**
- [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/blob/main/split_files/vae/ae.safetensors)

**Text Encoder**
- [clip_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors)
- [t5xxl_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors) or [t5xxl_fp8_e4m3fn_scaled.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors)

模型保存位置

```
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── [to be updated]
│   ├── 📂 vae/
│   │   └── ae.safetensor
│   └── 📂 text_encoders/
│       ├── clip_l.safetensors
│       └── t5xxl_fp16.safetensors 或者 t5xxl_fp8_e4m3fn_scaled.safetensors
```

### 3. 按步骤完成工作流的运行

![ComfyUI Flux Kontext 原生工作流步骤](/images/tutorial/flux/flux_1_kontext_step_guide.jpg)

你可参考图片中的序号来完成图工作流的运行：

<Note>
    目前我们在实际测试中，使用原始版本模型时，在 24GB 显存的 4090 显卡处理 1024x1024 分辨率上会出现显存不足的情况，在 40GB 显存的 A100 可以顺利运行 1024x1024 的图片。
    在此工作流中我们默认使用了 `fp8_e4m3fn` 精度设置来确保在普通消费级显卡上也可以运行，如果你需要尝试原始精度请修改为`default`
</Note>

1. 在 `Load Diffusion Model` 节点中加载 `flux_1_kontext_pro_image.safetensors` 模型，注意这里的 `weight_dtype` 已经被设置为 `fp8_e4m3fn` 来确保你在普通消费级显卡上也可以运行，如果你需要原始精度请修改为`default`
2. 在 `DualCLIP Load` 节点中确保： `clip_l.safetensors` 及 `t5xxl_fp16.safetensors` 或者 `t5xxl_fp8_e4m3fn_scaled.safetensors` 已经加载
3. 在 `Load VAE` 节点中确保加载 `ae.safetensors` 模型
4. 在 `Image size` 中设置尺寸
5. 在 `Load Image` 节点中加载需要编辑的图像
6. 在 `CLIP Text Encode` 节点中修改提示词，目前仅支持英文

<PromptTechniques/>