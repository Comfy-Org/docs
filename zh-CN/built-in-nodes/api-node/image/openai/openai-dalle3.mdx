---
title: "OpenAI DALL·E 3 - ComfyUI 原生节点文档"
description: "使用OpenAI的DALL·E 3模型生成高质量图像的节点"
sidebarTitle: "OpenAI DALL·E 3"
icon: "circle"
---

![ComfyUI 原生 Stability Stable Image Ultra 节点](/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg)

OpenAI DALL·E 3 节点允许你使用OpenAI的最新DALL·E 3模型，通过自然语言描述生成极其精确和高质量的图像。

## 节点功能

此节点连接到OpenAI的DALL·E 3 API，使用户能够通过详细的文本提示词生成高质量图像。DALL·E 3是OpenAI的最新图像生成模型，相比前代提供了显著提升的图像质量、更精确的提示词理解和更优秀的细节表现能力。

## 参数说明

### 基本参数

| 参数 | 类型 | 默认值 | 说明 |
|-----|-----|-------|------|
| prompt | 字符串 | "" | 详细描述要生成内容的文本提示词 |
| size | 选择项 | "1024x1024" | 输出图像尺寸 |
| quality | 选择项 | "standard" | 图像质量(standard或hd) |

### 可选参数

| 参数 | 类型 | 说明 |
|-----|-----|------|
| style | 选择项 | 视觉风格(vivid或natural) |
| seed | 整数 | 生成的随机种子 |
| n | 整数 | 生成的图像数量 |

### 输出

| 输出 | 类型 | 说明 |
|-----|-----|------|
| IMAGE | 图像 | 生成的图像结果 |


## 工作原理

OpenAI DALL·E 3 节点分析用户提供的文本提示词，通过OpenAI的DALL·E 3模型创建相应的图像。该模型利用先进的深度学习技术理解提示词的语义内容和视觉暗示，并将其转化为高质量图像。

用户可以通过设置不同的参数来控制生成结果，包括尺寸、质量级别和风格倾向。质量参数允许在生成速度和图像细节之间进行权衡，而风格选项则可以在生活化(natural)和生动(vivid)风格之间选择。

生成的图像链接有效期较短，建议及时下载或缓存重要的生成结果。

## 源码参考

[节点源码 (更新于2025-05-03)]

```python

class OpenAIDalle3(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's DALL·E 3 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for DALL·E",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "quality": (
                    IO.COMBO,
                    {
                        "options": ["standard", "hd"],
                        "default": "standard",
                        "tooltip": "Image quality",
                    },
                ),
                "style": (
                    IO.COMBO,
                    {
                        "options": ["natural", "vivid"],
                        "default": "natural",
                        "tooltip": "Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["1024x1024", "1024x1792", "1792x1024"],
                        "default": "1024x1024",
                        "tooltip": "Image size",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        style="natural",
        quality="standard",
        size="1024x1024",
        auth_token=None,
    ):
        model = "dall-e-3"

        # build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/openai/images/generations",
                method=HttpMethod.POST,
                request_model=OpenAIImageGenerationRequest,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=OpenAIImageGenerationRequest(
                model=model,
                prompt=prompt,
                quality=quality,
                size=size,
                style=style,
                seed=seed,
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
``` 